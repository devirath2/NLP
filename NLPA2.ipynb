{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fd91a7-8df5-46ca-be70-30d6581efd5c",
   "metadata": {},
   "source": [
    "## Task 1 DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd029c67-2fcd-4afd-8698-a40eec67fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   document_id                                               text\n",
      "0            1  The stock market has been experiencing volatil...\n",
      "1            2  The economy is growing, and businesses are opt...\n",
      "2            3  Climate change is a critical issue that needs ...\n",
      "3            4  Advances in artificial intelligence have revol...\n",
      "4            5  The rise of electric vehicles is shaping the f...\n",
      "\n",
      "Total number of rows: 10\n",
      "Unique documents: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"text_docs - text_docs.csv\")\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Print the total number of rows and unique documents\n",
    "total_rows = df.shape[0]\n",
    "unique_docs = df['document_id'].nunique()\n",
    "#Statistics: We calculate the total number of rows (shape[0]) and unique document IDs (nunique())\n",
    "#to better understand the dataset structure.\n",
    "\n",
    "print(f\"\\nTotal number of rows: {total_rows}\")\n",
    "print(f\"Unique documents: {unique_docs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9506208-b7e6-4e4b-8e0e-e9c661bf720a",
   "metadata": {},
   "source": [
    "## TASK2 GENERATE TOPICS USING LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919728b8-14dc-42a9-81ec-aa1509471061",
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 1 Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8105427-bd17-4702-a1ac-53b5179e366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4210fc0-68cf-44e3-8f0f-00b9ffb5467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the Document-Term Matrix: (10, 62)\n"
     ]
    }
   ],
   "source": [
    "##STEP2 CREATE DTM\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a document-term matrix (DTM)\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Check the shape of the DTM\n",
    "print(f\"\\nShape of the Document-Term Matrix: {dtm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df49a86-d1d9-4989-aca0-3dbcdac6a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #1:\n",
      "introduction treatments healthcare technologies evolving\n",
      "\n",
      "Topic #2:\n",
      "platforms digital change climate critical\n",
      "\n",
      "Topic #3:\n",
      "industry future due experiencing stock\n",
      "\n",
      "Topic #4:\n",
      "renewable world energy projects investing\n",
      "\n",
      "Topic #5:\n",
      "worldwide revolutionized artificial industries intelligence\n"
     ]
    }
   ],
   "source": [
    "#STEP3 APPLY LDA MODEL\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_top_words = 5\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic #{topic_idx+1}:\")\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e12e06-e969-4a79-bdf0-d609eb175045",
   "metadata": {},
   "source": [
    "LDA Model: The LDA model is trained using the LatentDirichletAllocation from sklearn. We specify n_components=5 to extract 5 topics.\n",
    "\n",
    "Top Words per Topic: For each topic, we display the top 5 words with the highest weights (importance). These words represent the essence of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d7226-db2c-4dba-a291-7d0443bf06fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
