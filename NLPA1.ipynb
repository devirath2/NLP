{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ac5452-36ef-40ea-922f-dce8ebf91a26",
   "metadata": {},
   "source": [
    "## TASK1 - DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e290a6d6-eeed-4b10-bceb-2c2a809b7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "                                                text     label\n",
      "0                 I loved the product, it's amazing!  positive\n",
      "1    Terrible service, I will never shop here again.  negative\n",
      "2    The quality is good, but the delivery was late.   neutral\n",
      "3  Absolutely wonderful experience, highly recomm...  positive\n",
      "4  Product was damaged when it arrived, very disa...  negative\n",
      "\n",
      "Total number of rows: 8\n",
      "Count of unique labels: 3\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Data Exploration\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the sample dataset\n",
    "df = pd.read_excel('text_class.xlsx')\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head()) ##head shows first 5 rows as default\n",
    "\n",
    "# Print total number of rows and count of unique labels\n",
    "print(\"\\nTotal number of rows:\", len(df))\n",
    "print(\"Count of unique labels:\", df['label'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d73afb-5b91-41f1-9792-8d5fdf47a787",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a87139-3855-45b4-9078-b30ec7b0b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\devir_jnfy7nx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devir_jnfy7nx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b67220-af95-4ea9-9ddf-c2e4546acfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed top 5 rows:\n",
      "                                                text  \\\n",
      "0                 I loved the product, it's amazing!   \n",
      "1    Terrible service, I will never shop here again.   \n",
      "2    The quality is good, but the delivery was late.   \n",
      "3  Absolutely wonderful experience, highly recomm...   \n",
      "4  Product was damaged when it arrived, very disa...   \n",
      "\n",
      "                                     processed_text  \n",
      "0                             loved product amazing  \n",
      "1                       terrible service never shop  \n",
      "2                        quality good delivery late  \n",
      "3  absolutely wonderful experience highly recommend  \n",
      "4              product damaged arrived disappointed  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the first 5 rows of text data\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first 5 rows of processed data\n",
    "print(\"\\nProcessed top 5 rows:\")\n",
    "print(df[['text', 'processed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b88b5-9792-4c4b-860b-94066162cf36",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Preprocessing Steps:\n",
    "\n",
    "Lowercasing: text.lower() converts all text to lowercase to ensure uniformity.\n",
    "\n",
    "Removing Punctuation: We loop through the text and remove any punctuation using string.punctuation and a list comprehension.\n",
    "\n",
    "Tokenization: word_tokenize(text) splits the text into individual words.\n",
    "\n",
    "Removing Stopwords: We remove common words (stopwords) like \"the\", \"is\", \"in\", etc., using NLTK's stopwords.words('english').\n",
    "\n",
    "Applying Preprocessing: The apply() function is used to apply the preprocessing function to the text column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b5c17-59c9-444b-8e62-6e7aa589371b",
   "metadata": {},
   "source": [
    "## Task 3 TRAIN A CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9311367e-45e5-4b59-9585-3b38d56950cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the Logistic Regression model: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "X = df['processed_text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to features using TF-IDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a simple Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy of the Logistic Regression model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c25afe-be2f-418c-a3a4-21eea65d5a96",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Data Splitting: The train_test_split() function splits the data into training (80%) and test (20%) sets.\n",
    "\n",
    "Feature Extraction (TF-IDF):\n",
    "\n",
    "The TfidfVectorizer() is used to convert the text data into numerical features (TF-IDF values).\n",
    "\n",
    "fit_transform() is applied on the training data to learn the features, and transform() is applied on the test data to use the learned features.\n",
    "\n",
    "Logistic Regression: We use a simple LogisticRegression() model to train the classifier on the training data.\n",
    "\n",
    "Accuracy Calculation: The accuracy is calculated using accuracy_score() by comparing the predicted labels (y_pred) with the actual labels (y_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc684b16-2586-48f5-a8fd-409c5581ecc0",
   "metadata": {},
   "source": [
    "##TASK 4 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31503426-8ec1-42a3-a345-9c0bbf573a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [1 0]]\n",
      "\n",
      "The confusion matrix provides insights into how well the model is performing by showing the number of true positives, true negatives, false positives, and false negatives. This allows us to understand where the model is making errors and how accurate it is in predicting each class.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries for confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the performance using confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Brief comment on confusion matrix:\n",
    "print(\"\\nThe confusion matrix provides insights by showing the number of true positives, true negatives, false positives, and false negatives. This allows us to understand where the model is making errors and how accurate it is in predicting each class.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e6aab-1b36-44eb-acc9-da24152598fc",
   "metadata": {},
   "source": [
    "## Final Thoughts and Insights:\n",
    "Model Evaluation: Based on the confusion matrix and accuracy score, you can decide if the model needs improvements or if different techniques (e.g., more data, different models, or hyperparameter tuning) should be explored.\n",
    "\n",
    "Dataset Limitation: A small dataset of only 8 samples may not give reliable model performance. The model's performance is likely to improve with a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec94dd-a4f4-4fe1-9d4f-5228f56435f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
